{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The project has evolved. The CSV files are now in the olist.db database. For the project with the old schema between the CSV files, it might be useful to create a mapping between the old CSV files and the database tables, as well as the common keys between the tables. https://drive.google.com/file/d/1cC1h5ZiakQMM6Ut9Hqf13r-jTpMHV8d5/view?usp=sharing  ",
   "id": "e8236cb396d6bd52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Schema of liaison between data when data where csv files see https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce",
   "id": "a4a75974a3adcc62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![data_schema.PNG](../data/data_schema.PNG)\n",
   "id": "ccfdf16ff715557f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here is a mapping table in text format between the CSV files and the tables in `olist.db`, including the identical keys between tables when they exist:\n",
    "\n",
    "| CSV File                               | SQLite Table       | Identical Keys                            |\n",
    "|----------------------------------------|--------------------|-------------------------------------------|\n",
    "| olist_customers_dataset.csv            | customers          | customer_id, customer_zip_code_prefix     |\n",
    "| olist_geolocation_dataset.csv          | geoloc             | geolocation_zip_code_prefix               |\n",
    "| olist_order_items_dataset.csv          | order_items        | order_id, product_id, seller_id           |\n",
    "| olist_order_payments_dataset.csv       | order_pymts        | order_id                                  |\n",
    "| olist_order_reviews_dataset.csv        | order_reviews      | order_id                                  |\n",
    "| olist_orders_dataset.csv               | orders             | customer_id, order_id                     |\n",
    "| olist_products_dataset.csv             | products           | product_id                                |\n",
    "| olist_sellers_dataset.csv              | sellers            | seller_id, seller_zip_code_prefix         |\n",
    "| product_category_name_translation.csv  | translation        | product_category_name                     |\n",
    "\n",
    "### Mapping Details\n",
    "\n",
    "1. **olist_customers_dataset.csv -> customers**\n",
    "   - Identical Keys: `customer_id`, `customer_zip_code_prefix`\n",
    "\n",
    "2. **olist_geolocation_dataset.csv -> geoloc**\n",
    "   - Identical Keys: `geolocation_zip_code_prefix`\n",
    "\n",
    "3. **olist_order_items_dataset.csv -> order_items**\n",
    "   - Identical Keys: `order_id`, `product_id`, `seller_id`\n",
    "\n",
    "4. **olist_order_payments_dataset.csv -> order_pymts**\n",
    "   - Identical Keys: `order_id`\n",
    "\n",
    "5. **olist_order_reviews_dataset.csv -> order_reviews**\n",
    "   - Identical Keys: `order_id`\n",
    "\n",
    "6. **olist_orders_dataset.csv -> orders**\n",
    "   - Identical Keys: `customer_id`, `order_id`\n",
    "\n",
    "7. **olist_products_dataset.csv -> products**\n",
    "   - Identical Keys: `product_id`\n",
    "\n",
    "8. **olist_sellers_dataset.csv -> sellers**\n",
    "   - Identical Keys: `seller_id`, `seller_zip_code_prefix`\n",
    "\n",
    "9. **product_category_name_translation.csv -> translation**\n",
    "   - Identical Keys: `product_category_name`\n",
    "\n",
    "### Explanation\n",
    "- Identical keys are used to establish relationships between different tables, thus facilitating the necessary joins for complex queries.\n",
    "- For example, `order_id` is a common key between the tables `orders`, `order_pymts`, `order_reviews`, and `order_items`, allowing the linking of order information, payment, reviews, and order items.\n",
    "\n",
    "This mapping table can serve as a reference to understand how the data from the CSV files is structured in the `olist.db` database and how they can be joined to answer analytical questions."
   ],
   "id": "e7c62d3056d4c224"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Olist E-Commerce connection to the database check",
   "id": "a0d5dda00b2e6cc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T07:23:16.613034Z",
     "start_time": "2024-07-17T07:23:16.584416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    print(\"Connecting to the database...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Database connected.\")\n",
    "    \n",
    "    query = \"SELECT COUNT(*) FROM orders;\"\n",
    "    try:\n",
    "        data = pd.read_sql(query, conn)\n",
    "        print(\"Total number of rows in orders table:\", data.iloc[0, 0])\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "else:\n",
    "    print(\"The file does not exist at the specified location:\", db_path)\n",
    "\n"
   ],
   "id": "d51e48109c77fa88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      "Database connected.\n",
      "Total number of rows in orders table: 99441\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d28929ddd889c526"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspecting the Database Schema",
   "id": "11b4ae36d392d9a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T08:36:12.311205Z",
     "start_time": "2024-07-17T08:36:12.265114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    print(\"Connecting to the database...\")\n",
    "    \n",
    "    # Create a connection to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Database connected.\")\n",
    "    \n",
    "    try:\n",
    "        # List all tables\n",
    "        tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "        tables = pd.read_sql(tables_query, conn)\n",
    "        print(\"Tables in the database:\")\n",
    "        print(tables)\n",
    "\n",
    "        # Inspect schema of the relevant tables\n",
    "        for table in tables['name']:\n",
    "            schema_query = f\"PRAGMA table_info({table});\"\n",
    "            schema = pd.read_sql(schema_query, conn)\n",
    "            print(f\"Schema of {table}:\")\n",
    "            print(schema)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "else:\n",
    "    print(\"The file does not exist at the specified location:\", db_path)\n",
    "\n"
   ],
   "id": "bc8840325bf30abf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      "Database connected.\n",
      "Tables in the database:\n",
      "            name\n",
      "0      customers\n",
      "1         geoloc\n",
      "2    order_items\n",
      "3    order_pymts\n",
      "4  order_reviews\n",
      "5         orders\n",
      "6       products\n",
      "7        sellers\n",
      "8    translation\n",
      "Schema of customers:\n",
      "   cid                      name    type  notnull dflt_value  pk\n",
      "0    0                     index  BIGINT        0       None   0\n",
      "1    1               customer_id    TEXT        0       None   0\n",
      "2    2        customer_unique_id    TEXT        0       None   0\n",
      "3    3  customer_zip_code_prefix  BIGINT        0       None   0\n",
      "4    4             customer_city    TEXT        0       None   0\n",
      "5    5            customer_state    TEXT        0       None   0\n",
      "Schema of geoloc:\n",
      "   cid                         name    type  notnull dflt_value  pk\n",
      "0    0                        index  BIGINT        0       None   0\n",
      "1    1  geolocation_zip_code_prefix  BIGINT        0       None   0\n",
      "2    2              geolocation_lat   FLOAT        0       None   0\n",
      "3    3              geolocation_lng   FLOAT        0       None   0\n",
      "4    4             geolocation_city    TEXT        0       None   0\n",
      "5    5            geolocation_state    TEXT        0       None   0\n",
      "Schema of order_items:\n",
      "   cid                 name    type  notnull dflt_value  pk\n",
      "0    0                index  BIGINT        0       None   0\n",
      "1    1             order_id    TEXT        0       None   0\n",
      "2    2        order_item_id  BIGINT        0       None   0\n",
      "3    3           product_id    TEXT        0       None   0\n",
      "4    4            seller_id    TEXT        0       None   0\n",
      "5    5  shipping_limit_date    TEXT        0       None   0\n",
      "6    6                price   FLOAT        0       None   0\n",
      "7    7        freight_value   FLOAT        0       None   0\n",
      "Schema of order_pymts:\n",
      "   cid                  name    type  notnull dflt_value  pk\n",
      "0    0                 index  BIGINT        0       None   0\n",
      "1    1              order_id    TEXT        0       None   0\n",
      "2    2    payment_sequential  BIGINT        0       None   0\n",
      "3    3          payment_type    TEXT        0       None   0\n",
      "4    4  payment_installments  BIGINT        0       None   0\n",
      "5    5         payment_value   FLOAT        0       None   0\n",
      "Schema of order_reviews:\n",
      "   cid                     name    type  notnull dflt_value  pk\n",
      "0    0                    index  BIGINT        0       None   0\n",
      "1    1                review_id    TEXT        0       None   0\n",
      "2    2                 order_id    TEXT        0       None   0\n",
      "3    3             review_score  BIGINT        0       None   0\n",
      "4    4     review_comment_title    TEXT        0       None   0\n",
      "5    5   review_comment_message    TEXT        0       None   0\n",
      "6    6     review_creation_date    TEXT        0       None   0\n",
      "7    7  review_answer_timestamp    TEXT        0       None   0\n",
      "Schema of orders:\n",
      "   cid                           name    type  notnull dflt_value  pk\n",
      "0    0                          index  BIGINT        0       None   0\n",
      "1    1                       order_id    TEXT        0       None   0\n",
      "2    2                    customer_id    TEXT        0       None   0\n",
      "3    3                   order_status    TEXT        0       None   0\n",
      "4    4       order_purchase_timestamp    TEXT        0       None   0\n",
      "5    5              order_approved_at    TEXT        0       None   0\n",
      "6    6   order_delivered_carrier_date    TEXT        0       None   0\n",
      "7    7  order_delivered_customer_date    TEXT        0       None   0\n",
      "8    8  order_estimated_delivery_date    TEXT        0       None   0\n",
      "Schema of products:\n",
      "   cid                        name    type  notnull dflt_value  pk\n",
      "0    0                       index  BIGINT        0       None   0\n",
      "1    1                  product_id    TEXT        0       None   0\n",
      "2    2       product_category_name    TEXT        0       None   0\n",
      "3    3         product_name_lenght   FLOAT        0       None   0\n",
      "4    4  product_description_lenght   FLOAT        0       None   0\n",
      "5    5          product_photos_qty   FLOAT        0       None   0\n",
      "6    6            product_weight_g   FLOAT        0       None   0\n",
      "7    7           product_length_cm   FLOAT        0       None   0\n",
      "8    8           product_height_cm   FLOAT        0       None   0\n",
      "9    9            product_width_cm   FLOAT        0       None   0\n",
      "Schema of sellers:\n",
      "   cid                    name    type  notnull dflt_value  pk\n",
      "0    0                   index  BIGINT        0       None   0\n",
      "1    1               seller_id    TEXT        0       None   0\n",
      "2    2  seller_zip_code_prefix  BIGINT        0       None   0\n",
      "3    3             seller_city    TEXT        0       None   0\n",
      "4    4            seller_state    TEXT        0       None   0\n",
      "Schema of translation:\n",
      "   cid                           name    type  notnull dflt_value  pk\n",
      "0    0                          index  BIGINT        0       None   0\n",
      "1    1          product_category_name    TEXT        0       None   0\n",
      "2    2  product_category_name_english    TEXT        0       None   0\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checking for Common Values in Key Columns",
   "id": "97edd3f0bda5c380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:29:46.652170Z",
     "start_time": "2024-07-18T14:29:39.871434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "# Function to execute SQL queries and return DataFrames\n",
    "def execute_query(query):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        return pd.read_sql(query, conn)\n",
    "\n",
    "# Function to get unique values of a column\n",
    "def get_unique_column_values(table, column):\n",
    "    query = f\"SELECT DISTINCT {column} FROM {table}\"\n",
    "    df = execute_query(query)\n",
    "    return set(df[column].values)\n",
    "\n",
    "# List of tables and columns to check\n",
    "tables_columns = {\n",
    "    'customers': ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix'],\n",
    "    'geoloc': ['geolocation_zip_code_prefix'],\n",
    "    'order_items': ['order_id', 'product_id', 'seller_id'],\n",
    "    'order_pymts': ['order_id'],\n",
    "    'order_reviews': ['order_id', 'review_id'],\n",
    "    'orders': ['order_id', 'customer_id'],\n",
    "    'products': ['product_id'],\n",
    "    'sellers': ['seller_id', 'seller_zip_code_prefix'],\n",
    "    'translation': ['product_category_name'],\n",
    "}\n",
    "\n",
    "# Function to compare columns and check for common values\n",
    "def compare_columns(tables_columns):\n",
    "    common_values_found = False\n",
    "    columns_to_compare = {}\n",
    "    \n",
    "    for table, columns in tables_columns.items():\n",
    "        for column in columns:\n",
    "            columns_to_compare[f\"{table}.{column}\"] = get_unique_column_values(table, column)\n",
    "    \n",
    "    keys = list(columns_to_compare.keys())\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        table_column1 = keys[i]\n",
    "        values1 = columns_to_compare[table_column1]\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            table_column2 = keys[j]\n",
    "            values2 = columns_to_compare[table_column2]\n",
    "            common_values = values1.intersection(values2)\n",
    "            if common_values:\n",
    "                print(f\"Common values found between {table_column1} and {table_column2}:\")\n",
    "                print(f\"Number of common values: {len(common_values)}\")\n",
    "                print(f\"Sample common values: {list(common_values)[:5]}\")  # Print only the first 5 common values as a sample\n",
    "                common_values_found = True\n",
    "    \n",
    "    if not common_values_found:\n",
    "        print(\"No common values found between the specified columns of different tables.\")\n",
    "\n",
    "# Specific validation for customer_id, order_id, and customer_unique_id\n",
    "def compare_specific_columns(columns_to_compare):\n",
    "    special_cases = [\n",
    "        ('customers.customer_id', 'orders.order_id'),\n",
    "        ('customers.customer_id', 'customers.customer_unique_id'),\n",
    "        ('customers.customer_unique_id', 'orders.order_id')\n",
    "    ]\n",
    "    \n",
    "    for case in special_cases:\n",
    "        table_column1, table_column2 = case\n",
    "        values1 = columns_to_compare.get(table_column1, set())\n",
    "        values2 = columns_to_compare.get(table_column2, set())\n",
    "        common_values = values1.intersection(values2)\n",
    "        if common_values:\n",
    "            print(f\"Common values found between {table_column1} and {table_column2}: {common_values}\")\n",
    "        else:\n",
    "            print(f\"No common values found between {table_column1} and {table_column2}\")    \n",
    "            \n",
    "\n",
    "# Include in main function\n",
    "def compare_columns(tables_columns):\n",
    "    common_values_found = False\n",
    "    columns_to_compare = {}\n",
    "    \n",
    "    for table, columns in tables_columns.items():\n",
    "        for column in columns:\n",
    "            columns_to_compare[f\"{table}.{column}\"] = get_unique_column_values(table, column)\n",
    "    \n",
    "    keys = list(columns_to_compare.keys())\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        table_column1 = keys[i]\n",
    "        values1 = columns_to_compare[table_column1]\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            table_column2 = keys[j]\n",
    "            values2 = columns_to_compare[table_column2]\n",
    "            common_values = values1.intersection(values2)\n",
    "            if common_values:\n",
    "                print(f\"Common values found between {table_column1} and {table_column2}:\")\n",
    "                print(f\"Number of common values: {len(common_values)}\")\n",
    "                print(f\"Sample common values: {list(common_values)[:5]}\")  # Print only the first 5 common values as a sample\n",
    "                common_values_found = True\n",
    "    \n",
    "    # Specific validation\n",
    "    if compare_specific_columns(columns_to_compare):\n",
    "        common_values_found = True\n",
    "    \n",
    "    if not common_values_found:\n",
    "        print(\"No common values found between the specified columns of different tables.\")\n",
    "\n",
    "# Execute the column comparison\n",
    "compare_columns(tables_columns)\n"
   ],
   "id": "f489e83e3b101b4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common values found between customers.customer_id and orders.customer_id:\n",
      "Number of common values: 99441\n",
      "Sample common values: ['79b14a47cb76b8a85cbb4fccf73a1b2b', '8b3b03fef783fb837ca0b86a72b2e202', '4f1f2b13805c2ab2ce70a6cad8001b18', '0040a8417928d0d5abd5169cd7877181', '201e36d57411ab16157cd234082bdad8']\n",
      "Common values found between customers.customer_zip_code_prefix and geoloc.geolocation_zip_code_prefix:\n",
      "Number of common values: 14837\n",
      "Sample common values: [65540, 65550, 65560, 98335, 98338]\n",
      "Common values found between customers.customer_zip_code_prefix and sellers.seller_zip_code_prefix:\n",
      "Number of common values: 2162\n",
      "Sample common values: [81925, 81930, 8215, 8220, 8223]\n",
      "Common values found between geoloc.geolocation_zip_code_prefix and sellers.seller_zip_code_prefix:\n",
      "Number of common values: 2239\n",
      "Sample common values: [81925, 81930, 8215, 8220, 8223]\n",
      "Common values found between order_items.order_id and order_pymts.order_id:\n",
      "Number of common values: 98665\n",
      "Sample common values: ['699787f56da4a1d2e5ef4ac575681986', 'c4f4ae572252215f366edb6f851c166f', 'f5f9034b5251b9a67898e5be6847e83c', '3452a7eb3fb50ae56f54d64d6ce7ddbd', '0877e8db9d779712c1c299b5bd8daa11']\n",
      "Common values found between order_items.order_id and order_reviews.order_id:\n",
      "Number of common values: 97917\n",
      "Sample common values: ['699787f56da4a1d2e5ef4ac575681986', 'c4f4ae572252215f366edb6f851c166f', 'f5f9034b5251b9a67898e5be6847e83c', '3452a7eb3fb50ae56f54d64d6ce7ddbd', '0877e8db9d779712c1c299b5bd8daa11']\n",
      "Common values found between order_items.order_id and orders.order_id:\n",
      "Number of common values: 98666\n",
      "Sample common values: ['699787f56da4a1d2e5ef4ac575681986', 'c4f4ae572252215f366edb6f851c166f', 'f5f9034b5251b9a67898e5be6847e83c', '3452a7eb3fb50ae56f54d64d6ce7ddbd', '0877e8db9d779712c1c299b5bd8daa11']\n",
      "Common values found between order_items.product_id and products.product_id:\n",
      "Number of common values: 32951\n",
      "Sample common values: ['884186aaa2ba70b64827a86966f583bd', '17865682ce7cccfe7dec02d1f74a8f86', '455819d0d2ccab9c6041aa8de3ae6b16', '264afa6d3b31bde30f0f2693c84795d9', 'd7f8eef8e05a45371252e633bb4f570c']\n",
      "Common values found between order_items.seller_id and sellers.seller_id:\n",
      "Number of common values: 3095\n",
      "Sample common values: ['ea3ebea5317b0efbc663ecc8ce5e9bc2', '010da0602d7774602cd1b3f5fb7b709e', '9da15f4a4ea758d9eeb49000dbe57e22', '3c3017960b2dbd72711a61f0400aab8f', '16bdc8cefd0e32a6f0824d296c5ad14a']\n",
      "Common values found between order_pymts.order_id and order_reviews.order_id:\n",
      "Number of common values: 98672\n",
      "Sample common values: ['699787f56da4a1d2e5ef4ac575681986', 'c4f4ae572252215f366edb6f851c166f', 'f5f9034b5251b9a67898e5be6847e83c', '3452a7eb3fb50ae56f54d64d6ce7ddbd', '1566cecea18d487c7958eb073713e8a7']\n",
      "Common values found between order_pymts.order_id and orders.order_id:\n",
      "Number of common values: 99440\n",
      "Sample common values: ['699787f56da4a1d2e5ef4ac575681986', 'c4f4ae572252215f366edb6f851c166f', 'f5f9034b5251b9a67898e5be6847e83c', '3452a7eb3fb50ae56f54d64d6ce7ddbd', '0877e8db9d779712c1c299b5bd8daa11']\n",
      "Common values found between order_reviews.order_id and orders.order_id:\n",
      "Number of common values: 98673\n",
      "Sample common values: ['699787f56da4a1d2e5ef4ac575681986', 'c4f4ae572252215f366edb6f851c166f', 'f5f9034b5251b9a67898e5be6847e83c', '3452a7eb3fb50ae56f54d64d6ce7ddbd', '1566cecea18d487c7958eb073713e8a7']\n",
      "No common values found between customers.customer_id and orders.order_id\n",
      "No common values found between customers.customer_id and customers.customer_unique_id\n",
      "No common values found between customers.customer_unique_id and orders.order_id\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Expected Foreign Key Relationships**:\n",
    "    - `order_items.order_id`, `order_pymts.order_id`, `order_reviews.order_id`, and `orders.order_id`: High numbers of common values, indicating expected foreign key relationships.\n",
    "    - `order_items.product_id` and `products.product_id`: 32951 common values, indicating a correct relationship.\n",
    "    - `order_items.seller_id` and `sellers.seller_id`: 3095 common values, indicating a correct relationship.\n",
    "\n",
    "2. **Zip Code Relationships**:\n",
    "    - `customers.customer_zip_code_prefix` and `geoloc.geolocation_zip_code_prefix`: 14837 common values.\n",
    "    - `customers.customer_zip_code_prefix` and `sellers.seller_zip_code_prefix`: 2162 common values.\n",
    "    - `geoloc.geolocation_zip_code_prefix` and `sellers.seller_zip_code_prefix`: 2239 common values.\n",
    "\n",
    "### Specific Validation\n",
    "\n",
    "1. **No Common Values**:\n",
    "    - `customers.customer_id` and `orders.order_id`: No common values found.\n",
    "    - `customers.customer_id` and `customers.customer_unique_id`: No common values found.\n",
    "    - `customers.customer_unique_id` and `orders.order_id`: No common values found.\n",
    "\n",
    "This indicates that the `customer_id` in the `customers` table is properly distinguished from `order_id` in the `orders` table and `customer_unique_id` in the `customers` table. This is the expected result and confirms that there are no unexpected overlaps between these IDs.\n",
    "\n",
    "-  Common values found between customers.customer_id and orders.customer_id: Number of common values: 99441 => same values so"
   ],
   "id": "abad93b3d38bee8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Query to Display a Few Rows and the Date Format",
   "id": "cd03086b50af2d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      "Database connected.\n",
      "Sample rows from orders table:\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp  \n",
      "0    delivered      2017-10-02 10:56:33  \n",
      "1    delivered      2018-07-24 20:41:37  \n",
      "2    delivered      2018-08-08 08:38:49  \n",
      "3    delivered      2017-11-18 19:28:06  \n",
      "4    delivered      2018-02-13 21:18:39  \n",
      "Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    print(\"Connecting to the database...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Database connected.\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_status,\n",
    "        order_purchase_timestamp\n",
    "    FROM orders\n",
    "    LIMIT 5;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_sql(query, conn)\n",
    "        print(\"Sample rows from orders table:\")\n",
    "        print(data)  # Display sample rows to check date format\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "else:\n",
    "    print(\"The file does not exist at the specified location:\", db_path)\n"
   ],
   "id": "67d995ae11d4105d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Query to Display the Current Date in order_purchase_timestamp Format",
   "id": "7d3fff2c73d5560d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      "Database connected.\n",
      "Current date (formatted):\n",
      "          current_date\n",
      "0  2024-07-17 07:52:15\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    print(\"Connecting to the database...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Database connected.\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT strftime('%Y-%m-%d %H:%M:%S', 'now') AS current_date;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_sql(query, conn)\n",
    "        print(\"Current date (formatted):\")\n",
    "        print(data)  # Display the current date to ensure it's being calculated correctly\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "else:\n",
    "    print(\"The file does not exist at the specified location:\", db_path)\n"
   ],
   "id": "789dee101efc8dae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Recent Orders with at Least 3 Days of Delay (Excluding Canceled Orders) for Orders Less Than 90 Days Old (urgent requests) ?",
   "id": "f8a332cb2ff68c1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Query\n",
    "1. **Recent Orders**: The query filters the orders based on the purchase timestamp to select only the recent orders placed in the last 90 days.\n",
    "2. **Order Status**: It excludes the canceled orders by checking the order status.\n",
    "3. **Delay Calculation**: The delay in delivery is calculated by finding the difference in days between the delivered customer date and the estimated delivery date.\n",
    "4. **Common Table Expression (CTE)**: The query uses a Common Table Expression (CTE) named `RelevantOrders` to filter the orders based on the conditions specified.\n",
    "5. **Result Columns**: The final result includes the `order_id`, `customer_id`, `order_status`, `order_purchase_timestamp`, `order_delivered_customer_date`, and `delay_days` columns for the selected orders.\n",
    "6. **Data Loading**: The query result is loaded into a pandas DataFrame for further analysis and display.\n",
    "7. **Display**: The first few rows of the resulting DataFrame are displayed to show the recent orders with at least 3 days of delay.\n",
    "8. **Context**: The query considers the date range issue and provides additional information if no data is available due to older dates in the database."
   ],
   "id": "4ba3f6cb18c279d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:38:09.045901Z",
     "start_time": "2024-07-18T08:38:07.340127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "# Function to execute SQL queries and return results as a pandas DataFrame\n",
    "def execute_query(query):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        return pd.read_sql(query, conn)\n",
    "\n",
    "# Check if the file exists before attempting to connect\n",
    "if os.path.exists(db_path):\n",
    "    print(\"Connecting to the database...\")\n",
    "    \n",
    "    # Create a connection to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(\"Database connected.\")\n",
    "    \n",
    "    # SQL query to count recent orders\n",
    "    count_query = \"\"\"\n",
    "    SELECT COUNT(*) AS recent_orders_count\n",
    "    FROM orders\n",
    "    WHERE \n",
    "        order_status <> 'canceled' AND \n",
    "        order_purchase_timestamp >= datetime('now', '-90 days');\n",
    "    \"\"\"\n",
    "    \n",
    "    # SQL query to retrieve recent orders with delay using a Common Table Expression (CTE); WITH AS creates temporary table in memory\n",
    "    # In our case, RelevantOrders is a temporary table that filters orders based on the conditions specified\n",
    "    data_query = \"\"\"\n",
    "    WITH RelevantOrders AS (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            order_status,\n",
    "            order_purchase_timestamp,\n",
    "            order_delivered_customer_date,\n",
    "            julianday(order_delivered_customer_date) - julianday(order_estimated_delivery_date) AS delay_days\n",
    "        FROM orders\n",
    "        WHERE \n",
    "            order_status <> 'canceled' AND \n",
    "            order_purchase_timestamp >= datetime('now', '-90 days')\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM RelevantOrders\n",
    "    WHERE delay_days > 3;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Execute the count query using the function and print the result\n",
    "        count_data = execute_query(count_query)\n",
    "        recent_orders_count = count_data.iloc[0, 0]\n",
    "        print(\"Number of recent orders in the last 90 days:\", recent_orders_count)\n",
    "        \n",
    "        # Execute the data query using the function and load data into a DataFrame\n",
    "        data = execute_query(data_query)\n",
    "        if recent_orders_count == 0 or data.empty:\n",
    "            print(f\"No data available for recent orders with at least 3 days of delay (excluding canceled orders) for orders less than 90 days old. This may be due to the date range issue because today's date is {today}, and the data in the database may be older.\")\n",
    "        else:\n",
    "            print(\"Query executed successfully.\")\n",
    "            print(data.head())  # Display the first few rows of the result\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "else:\n",
    "    print(\"The file does not exist at the specified location:\", db_path)\n"
   ],
   "id": "70f0afd505c947c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      "Database connected.\n",
      "Number of recent orders in the last 90 days: 0\n",
      "No data available for recent orders with at least 3 days of delay (excluding canceled orders) for orders less than 90 days old. This may be due to the date range issue because today's date is 2024-07-18, and the data in the database may be older.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Sellers generating revenue over 100,000 Real via Olist?",
   "id": "86a48f30ef5d9113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Query\n",
    "1. **Revenue Calculation**: The query calculates the total revenue generated by each seller by summing the prices of the products they sold. It uses the `order_items` table to get the price of each product and the `orders` table to filter only the orders that are delivered (`order_status = 'delivered'`).\n",
    "2. **Common Key**: The common key between the `order_items` and `sellers` tables is the `seller_id`, which is used to join the two tables and retrieve additional information about the sellers.\n",
    "3. **Filtering Criteria**: The query filters the sellers based on the total revenue generated, selecting only those sellers who have generated over 100,000 Real.\n",
    "4. **Result Columns**: The final result includes the `seller_id`, `seller_zip_code_prefix`, and `total_revenue` columns for the selected sellers.\n",
    "5. **Data Loading**: The query result is loaded into a pandas DataFrame for further analysis and display.\n",
    "6. **Display**: The first few rows of the resulting DataFrame are displayed to show the sellers who have generated revenue over 100,000 Real.\n",
    "7. **Context**: The query considers the date range issue and provides additional information if no data is available due to older dates in the database."
   ],
   "id": "a757012e6a77489d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:16.756343Z",
     "start_time": "2024-07-17T10:29:13.872919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "# Function to execute SQL queries and return DataFrames\n",
    "def execute_query(query):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        return pd.read_sql(query, conn)\n",
    "\n",
    "# SQL query for sellers generating revenue over 100,000 Real\n",
    "query_revenue_sellers = \"\"\"\n",
    "WITH Revenue AS (\n",
    "    SELECT\n",
    "        i.seller_id,\n",
    "        SUM(i.price) AS total_revenue\n",
    "    FROM order_items AS i\n",
    "    JOIN orders AS o ON i.order_id = o.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    GROUP BY i.seller_id\n",
    ")\n",
    "SELECT s.seller_id, s.seller_zip_code_prefix, r.total_revenue\n",
    "FROM Revenue AS r\n",
    "JOIN sellers AS s ON r.seller_id = s.seller_id\n",
    "WHERE r.total_revenue > 100000;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "data_revenue_sellers = execute_query(query_revenue_sellers)\n",
    "print(\"DataFrame: revenue_sellers\")\n",
    "print(data_revenue_sellers.head())\n",
    "print()\n"
   ],
   "id": "210c04c782ad1f36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: revenue_sellers\n",
      "                          seller_id  seller_zip_code_prefix  total_revenue\n",
      "0  7e93a43ef30c4f03f38b393420bc753a                    6429      165981.49\n",
      "1  7d13fca15225358621be4086e1eb0964                   14050      112436.18\n",
      "2  955fee9216a65b617aa5c0531780ce60                    4782      131836.71\n",
      "3  1f50f920176fa81dab994f9023523100                   15025      106655.71\n",
      "4  fa1c13f2614d7b5c4749cbc52fecda94                   13170      190917.14\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.Who are the new sellers (less than 3 months of seniority) who are already highly engaged with the platform (having already sold more than 30 products)?",
   "id": "e012f47a241cfcba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Query\n",
    "1. **New Sellers**: The query identifies new sellers who have been active on the platform for less than 90 days. It calculates the seller's seniority based on the date of their first order.\n",
    "2. **Highly Engaged Sellers**: The query filters the new sellers who have already sold more than 30 products, indicating high engagement with the platform.\n",
    "3. **Common Table Expressions (CTEs)**: The query uses two Common Table Expressions (CTEs) to first find the first order date for each seller and then calculate the total number of products sold by each seller.\n",
    "4. **Result Columns**: The final result includes the `seller_id` and `seller_zip_code_prefix` columns for the selected new sellers who are highly engaged with the platform.\n",
    "5. **Data Loading**: The query result is loaded into a pandas DataFrame for further analysis and display.\n",
    "6. **Display**: The first few rows of the resulting DataFrame are displayed to show the new sellers who are highly engaged with the platform.\n",
    "7. **Context**: The query considers the date range issue and provides additional information if no data is available due to older dates in the database."
   ],
   "id": "85add89c7680916e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:17:49.732802Z",
     "start_time": "2024-07-18T09:17:46.680846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "# Function to execute SQL queries and return DataFrames\n",
    "def execute_query(query):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        return pd.read_sql(query, conn)\n",
    "\n",
    "# Step 1: Check Min and Max Dates in the Orders Table (Optional for context)\n",
    "query_check_dates = \"\"\"\n",
    "SELECT \n",
    "    MIN(order_purchase_timestamp) AS min_date,\n",
    "    MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders;\n",
    "\"\"\"\n",
    "\n",
    "data_check_dates = execute_query(query_check_dates)\n",
    "print(\"DataFrame: check_dates\")\n",
    "print(data_check_dates)\n",
    "if data_check_dates.empty:\n",
    "    print(f\"No data available. This may be due to the date range issue because today's date is {today}, and the data in the database are older than 30 days.\")\n",
    "print()\n",
    "\n",
    "# Step 2: Check Distribution of Dates (Optional for context)\n",
    "query_distribution_dates = \"\"\"\n",
    "SELECT \n",
    "    order_purchase_timestamp,\n",
    "    COUNT(*) AS order_count\n",
    "FROM orders\n",
    "GROUP BY order_purchase_timestamp\n",
    "ORDER BY order_purchase_timestamp;\n",
    "\"\"\"\n",
    "\n",
    "data_distribution_dates = execute_query(query_distribution_dates)\n",
    "print(\"DataFrame: distribution_dates\")\n",
    "print(data_distribution_dates.head(10))  # Display the first 10 rows\n",
    "if data_distribution_dates.empty:\n",
    "    print(f\"No data available. This may be due to the date range issue because today's date is {today}, and the data in the database are older than 30 days.\")\n",
    "print()\n",
    "\n",
    "# Step 3: Check Earliest Order Dates for Sellers (Optional for context)\n",
    "query_earliest_order_dates = \"\"\"\n",
    "SELECT \n",
    "    i.seller_id,\n",
    "    MIN(o.order_purchase_timestamp) AS first_order_date\n",
    "FROM order_items AS i\n",
    "JOIN orders AS o ON i.order_id = o.order_id\n",
    "GROUP BY i.seller_id\n",
    "ORDER BY first_order_date DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "data_earliest_order_dates = execute_query(query_earliest_order_dates)\n",
    "print(\"DataFrame: earliest_order_dates\")\n",
    "print(data_earliest_order_dates)\n",
    "if data_earliest_order_dates.empty:\n",
    "    print(f\"No data available. This may be due to the date range issue because today's date is {today}, and the data in the database are older than 30 days.\")\n",
    "print()\n",
    "\n",
    "# Final Query: New Sellers with More Than 30 Products Sold in the Last 90 Days\n",
    "query_new_sellers_final = \"\"\"\n",
    "WITH SellerFirstOrder AS (\n",
    "    SELECT \n",
    "        i.seller_id,\n",
    "        MIN(o.order_purchase_timestamp) AS first_order_date\n",
    "    FROM order_items AS i\n",
    "    JOIN orders AS o ON i.order_id = o.order_id\n",
    "    GROUP BY i.seller_id\n",
    "),\n",
    "NewSellers AS (\n",
    "    SELECT \n",
    "        sfo.seller_id,\n",
    "        COUNT(i.order_item_id) AS total_products_sold\n",
    "    FROM SellerFirstOrder AS sfo\n",
    "    JOIN order_items AS i ON sfo.seller_id = i.seller_id\n",
    "    JOIN orders AS o ON i.order_id = o.order_id\n",
    "    WHERE \n",
    "        julianday('now') - julianday(sfo.first_order_date) < 90\n",
    "    GROUP BY sfo.seller_id\n",
    "    HAVING total_products_sold > 30\n",
    ")\n",
    "SELECT s.seller_id, s.seller_zip_code_prefix\n",
    "FROM NewSellers AS ns\n",
    "JOIN sellers AS s ON ns.seller_id = s.seller_id;\n",
    "\"\"\"\n",
    "\n",
    "data_new_sellers_final = execute_query(query_new_sellers_final)\n",
    "print(\"DataFrame: new_sellers_final\")\n",
    "print(data_new_sellers_final)\n",
    "if data_new_sellers_final.empty:\n",
    "    print(f\"No data available. This may be due to the date range issue because today's date is {today}, and the data in the database are older than 30 days.\")\n",
    "print()\n"
   ],
   "id": "bf792b10a1a887a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: check_dates\n",
      "              min_date             max_date\n",
      "0  2016-09-04 21:15:19  2018-10-17 17:30:18\n",
      "\n",
      "DataFrame: distribution_dates\n",
      "  order_purchase_timestamp  order_count\n",
      "0      2016-09-04 21:15:19            1\n",
      "1      2016-09-05 00:15:34            1\n",
      "2      2016-09-13 15:24:19            1\n",
      "3      2016-09-15 12:16:38            1\n",
      "4      2016-10-02 22:07:52            1\n",
      "5      2016-10-03 09:44:50            1\n",
      "6      2016-10-03 16:56:50            1\n",
      "7      2016-10-03 21:01:41            1\n",
      "8      2016-10-03 21:13:36            1\n",
      "9      2016-10-03 22:06:03            1\n",
      "\n",
      "DataFrame: earliest_order_dates\n",
      "                          seller_id     first_order_date\n",
      "0  6561d6bf844e464b4019442692b40e02  2018-08-28 09:26:43\n",
      "1  3296662b1331dea51e744505065ae889  2018-08-27 12:41:49\n",
      "2  e8ff5a6ceb895583033fc2a0f314e3c2  2018-08-26 14:17:08\n",
      "3  b76f4d90e85657a240495c876313adc5  2018-08-25 22:28:18\n",
      "4  26e2e5033827d2ba53929f43e03d8ffe  2018-08-25 12:50:59\n",
      "5  edb58a1390adf273840030a3d6253829  2018-08-25 11:15:44\n",
      "6  e0487761face83d64fcada2408959a36  2018-08-24 15:05:27\n",
      "7  1fa2d3def6adfa70e58c276bb64fe5bb  2018-08-24 13:04:05\n",
      "8  0af2ab31141893d26aca5a404a537dab  2018-08-23 22:37:44\n",
      "9  6c77cd583b36a13aacccf2b3ee23e233  2018-08-23 21:45:16\n",
      "\n",
      "DataFrame: new_sellers_final\n",
      "Empty DataFrame\n",
      "Columns: [seller_id, seller_zip_code_prefix]\n",
      "Index: []\n",
      "No data available. This may be due to the date range issue because today's date is 2024-07-18, and the data in the database are older than 30 days.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Which are the 5 zip codes with more than 30 reviews that have the worst average review scores over the last 12 months?",
   "id": "f43d99a67da6e69a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Query\n",
    "1. **Average Review Scores**: The query calculates the average review scores for each zip code based on the reviews received over the last 12 months.\n",
    "2. **Filtering Criteria**: The query filters the zip codes based on the number of reviews received, selecting only those zip codes with more than 30 reviews.\n",
    "3. **Common Table Expression (CTE)**: The query uses a Common Table Expression (CTE) named `RecentReviews` to filter the reviews based on the review creation date within the last 12 months.\n",
    "4. **Result Columns**: The final result includes the `customer_zip_code_prefix` and `average_score` columns for the selected zip codes.\n",
    "5. **Data Loading**: The query result is loaded into a pandas DataFrame for further analysis and display.\n",
    "6. **Display**: The first few rows of the resulting DataFrame are displayed to show the 5 zip codes with the worst average review scores over the last 12 months.\n",
    "7. **Context**: The query considers the date range issue and provides additional information if no data is available due to older dates in the database."
   ],
   "id": "dd1a821ff9045c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:38:52.499916Z",
     "start_time": "2024-07-18T09:38:52.373385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Relative path to the database\n",
    "db_path = os.path.join('..', 'data', 'olist.db')\n",
    "\n",
    "# Function to execute SQL queries and return DataFrames\n",
    "def execute_query(query):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        return pd.read_sql(query, conn)\n",
    "\n",
    "# SQL query to find the 5 zip codes with the worst average review scores over the last 12 months\n",
    "query_zip_codes_reviews = \"\"\"\n",
    "WITH RecentReviews AS (\n",
    "    SELECT \n",
    "        r.review_id,\n",
    "        r.review_score,\n",
    "        o.order_id,\n",
    "        c.customer_zip_code_prefix,\n",
    "        r.review_creation_date\n",
    "    FROM order_reviews AS r\n",
    "    JOIN orders AS o ON r.order_id = o.order_id\n",
    "    JOIN customers AS c ON o.customer_id = c.customer_id\n",
    "    WHERE \n",
    "        julianday('now') - julianday(r.review_creation_date) <= 365\n",
    "),\n",
    "ZipScores AS (\n",
    "    SELECT \n",
    "        customer_zip_code_prefix,\n",
    "        AVG(review_score) AS average_score,\n",
    "        COUNT(review_id) AS review_count\n",
    "    FROM RecentReviews\n",
    "    GROUP BY customer_zip_code_prefix\n",
    "    HAVING review_count > 30\n",
    ")\n",
    "SELECT \n",
    "    customer_zip_code_prefix,\n",
    "    average_score\n",
    "FROM ZipScores\n",
    "ORDER BY average_score ASC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "data_zip_codes_reviews = execute_query(query_zip_codes_reviews)\n",
    "print(\"DataFrame: zip_codes_reviews\")\n",
    "print(data_zip_codes_reviews)\n",
    "\n",
    "# Check if the DataFrame is empty and provide additional date information\n",
    "if data_zip_codes_reviews.empty:\n",
    "    # Query to get the most recent review date\n",
    "    query_recent_review_date = \"\"\"\n",
    "    SELECT MAX(review_creation_date) AS most_recent_review_date\n",
    "    FROM order_reviews;\n",
    "    \"\"\"\n",
    "    recent_review_date = execute_query(query_recent_review_date)\n",
    "    most_recent_date = recent_review_date['most_recent_review_date'].iloc[0]\n",
    "    \n",
    "    print(f\"No data available. This may be due to the date range issue because today's date is {today}, and the data in the database are older than 12 months.\")\n",
    "    print(f\"The most recent review date in the database is {most_recent_date}.\")\n",
    "\n",
    "print()\n"
   ],
   "id": "1746923b3cc4b1e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: zip_codes_reviews\n",
      "Empty DataFrame\n",
      "Columns: [customer_zip_code_prefix, average_score]\n",
      "Index: []\n",
      "No data available. This may be due to the date range issue because today's date is 2024-07-18, and the data in the database are older than 12 months.\n",
      "The most recent review date in the database is 2018-08-31 00:00:00.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
